{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205184e5",
   "metadata": {},
   "source": [
    "# 1. Inicializando projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "407b0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\"../../.env\")\n",
    "\n",
    "pinecone_api_ley = config[\"PINECONE_API_KEY\"]\n",
    "pinecone_env = config[\"PINECONE_ENVIRONMENT\"]\n",
    "index_pinecone = config[\"INDEX_NAME\"]\n",
    "\n",
    "openai_api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4222c6",
   "metadata": {},
   "source": [
    "## 1.1 Inicializar cliente Pinecone (configurar conexão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e356faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "    \"name\": \"livreto-base-evidencia\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"livreto-base-evidencia-y4phmo4.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"region\": \"us-east-1\",\n",
      "            \"cloud\": \"aws\",\n",
      "            \"read_capacity\": {\n",
      "                \"mode\": \"OnDemand\",\n",
      "                \"status\": {\n",
      "                    \"state\": \"Ready\",\n",
      "                    \"current_shards\": null,\n",
      "                    \"current_replicas\": null\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 1536,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": {\n",
      "        \"embedding_model\": \"text-embedding-3-small\"\n",
      "    }\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_ley)\n",
    "index = pc.Index(index_pinecone)\n",
    "\n",
    "# Verificando se está tudo ok\n",
    "print(pc.list_indexes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9ff9d",
   "metadata": {},
   "source": [
    "# 2. Extrair texto do PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03412fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe? False\n",
      "Caminho absoluto: C:\\Users\\irani\\Desktop\\Personal code\\LLM\\Using LLM\\LangChain\\RAG\\LangChain\\RAG\\Data\\Livreto_BaseEvidencia_2019.11.21.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"LangChain/RAG/Data/Livreto_BaseEvidencia_2019.11.21.pdf\")\n",
    "print(\"Existe?\", path.exists())\n",
    "print(\"Caminho absoluto:\", path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d854c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "path = \"Data/Livreto_BaseEvidencia_2019.11.21.pdf\"\n",
    "pages=[]\n",
    "with pdfplumber.open(path) as pdf:\n",
    "    for i , p in enumerate(pdf.pages, start=1):\n",
    "        text = p.extract_text() or \"\"\n",
    "        pages.append({\"page\": i, \"text\": text})\n",
    "\n",
    "with open(\"data/pdf_pages_livreto.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(pages,f,ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd7470",
   "metadata": {},
   "source": [
    "# Limpeza e normalização de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb95af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpar_texto(texto: str) -> str:\n",
    "    if not texto:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove caracteres estranhos comuns de OCR\n",
    "    texto = re.sub(r'[^\\wÀ-ÿ\\s.,;:!?()\\-\"“”]', ' ', texto)\n",
    "\n",
    "    # Remove letras soltas em linhas (ex: O\\nI\\nR\\n)\n",
    "    texto = re.sub(r'(?:\\b[A-ZÀ-Ý]\\b\\s*){3,}', ' ', texto)\n",
    "\n",
    "    # Normaliza quebras de linha\n",
    "    texto = re.sub(r'\\n{2,}', '\\n', texto)\n",
    "    texto = re.sub(r'\\n', ' ', texto)\n",
    "\n",
    "    # Normaliza espaços\n",
    "    texto = re.sub(r'\\s{2,}', ' ', texto)\n",
    "\n",
    "    return texto.strip()\n",
    "\n",
    "def limpar_json_paginas(paginas: list) -> list:\n",
    "    json_limpo = []\n",
    "\n",
    "    for pagina in paginas:\n",
    "        texto_limpo = limpar_texto(pagina.get(\"text\", \"\"))\n",
    "\n",
    "        # ignora páginas vazias após limpeza\n",
    "        if texto_limpo:\n",
    "            json_limpo.append({\n",
    "                \"page\": pagina[\"page\"],\n",
    "                \"text\": texto_limpo\n",
    "            })\n",
    "\n",
    "    return json_limpo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd9dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/pdf_pages_livreto.json\", encoding=\"utf-8\") as f:\n",
    "    paginas = json.load(f)\n",
    "\n",
    "json_limpo = limpar_json_paginas(paginas)\n",
    "\n",
    "with open(\"data/pdf_pages_livreto_clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_limpo, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca9aed",
   "metadata": {},
   "source": [
    "# Dividir texto em pedaços menores (Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faedbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dados\n",
    "from langchain_core.documents import Document\n",
    "with open('data/pdf_pages_livreto_clean.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "paginas_texto = {}\n",
    "for item in data:\n",
    "    p = item['page']\n",
    "    t = item['text']\n",
    "    if p not in paginas_texto:\n",
    "        paginas_texto[p] = t\n",
    "    else:\n",
    "        # Une fragmentos da mesma página com uma quebra de linha\n",
    "        paginas_texto[p] += \"\\n\" + t\n",
    "\n",
    "# Convertendo para objetos Document do LangChain (Lista de objetos consolidados)\n",
    "documents = [\n",
    "    Document(page_content=texto, metadata={\"page\": pagina}) \n",
    "    for pagina, texto in paginas_texto.items()\n",
    "]\n",
    "\n",
    "\n",
    "# Estratégia de Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, # Suficiente para captar o tópico inteiro de uma página\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"I DEFINIÇÃO\", \"II MOBILIZAÇÃO\", \"III DETERMINANTES\", \n",
    "                \"IV SOLUÇÃO\", \"V JUSTIFICATIVA\", \"VI APRIMORAMENTO\", \n",
    "                \"VII CERTIFICAÇÃO\", \"\\n\\n\", \". \"]\n",
    ")\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61eb1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conferindo 'a cara' dos chunks!\n",
      "V JUSTIFICATIVA Toda solução demanda recursos. Para ser justificada, uma solução precisa que o valor do que é capaz de alcançar (seus benefícios) supere o valor.dos recursos que necessita (seus custos). Embora a justificativa definitiva de uma solução só possa ocorrer após a sua impantação, a decisão por adotá-la precisa ser baseada em avaliações ex-ante de sua relação custo-efetividade e custo-benefício. AVITACIFITSUJ “If whatever it is you re explaining has some measure ... you ll be much better able to discriminate among competing hypotheses. What is vague ... is open to many explanations” Carl Sagan\n"
     ]
    }
   ],
   "source": [
    "print(\"Conferindo 'a cara' dos chunks!\")\n",
    "print(chunks[13].page_content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69286ce6",
   "metadata": {},
   "source": [
    "# Embeddings + Ingestão no Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef2a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Inicializando o modelo de embeddings da OpenAI\n",
    "# O modelo 'text-embedding-3-small' é o mais custo-benefício atualmente\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=openai_api_key, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4882be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso! 22 documentos foram indexados no Pinecone.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Enviando os chunks (sua lista de Document) para o Pinecone\n",
    "# O LangChain vai automaticamente gerar os embeddings e salvar no Index\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=chunks, # Usando a variável 'chunks'\n",
    "    embedding=embeddings_model,\n",
    "    index_name=index_pinecone,\n",
    "    pinecone_api_key=pinecone_api_ley\n",
    ")\n",
    "\n",
    "print(f\"Sucesso! {len(chunks)} documentos foram indexados no Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b709f6",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "127d58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classificação Sugerida ---\n",
      "Conteúdo do Guia: V JUSTIFICATIVA Valoração do custo Impacto (eficácia) Recursos são sempre escassos. Assim, justificar, com base em evidência a adoção de uma ação requer tanto estimativas da magnitude do seu impacto q...\n",
      "Página Original: 16\n"
     ]
    }
   ],
   "source": [
    "# Texto de um novo documento que você deseja classificar\n",
    "query = \"\"\"\n",
    "O orçamento de 2026 prevê cortes severos nas bolsas da CAPES e CNPq, \n",
    "com reduções de até 18% e 25%, respectivamente. A Lei Orçamentária Anual (LOA) \n",
    "de 2026 aponta uma queda de R$ 359,3 milhões para a CAPES. Esses recursos \n",
    "foram redirecionados para emendas parlamentares em ano eleitoral, \n",
    "gerando grande preocupação na comunidade acadêmica quanto à formação de pesquisadores.\"\"\"\n",
    "\n",
    "# Busca os documentos mais parecidos\n",
    "docs = vector_store.similarity_search(query, k=1)\n",
    "\n",
    "if docs:\n",
    "    print(\"\\n--- Classificação Sugerida ---\")\n",
    "    print(f\"Conteúdo do Guia: {docs[0].page_content[:200]}...\")\n",
    "    print(f\"Página Original: {docs[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cb58e",
   "metadata": {},
   "source": [
    "# Consulta e recuperação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b651f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o retriever (configurado para trazer o chunk mais relevante)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# O retriever é um motor de busca.\n",
    "# 1. Ele transforma a sua query_usuario em um vetor.\n",
    "# 2. Ele vai ao Pinecone e procura o chunk que tem a maior \"similaridade de cosseno\" (o texto mais parecido semanticamente).\n",
    "# 3. k=1: Você configurou para ele trazer apenas o melhor resultado. Se o usuário fala de \"orçamento\", o retriever traz a página de custos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Nome da Etapa: V JUSTIFICATIVA\n",
      "2. Justificativa curta: O texto aborda a valoração de custos e o impacto de cortes orçamentários, elementos centrais na justificativa de políticas.\n",
      "3. Página de referência do guia original: 16.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Definimos o modelo (LLM)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key, temperature=0)\n",
    "\n",
    "# 2. Criamos o template com 'espaços vazios' destinados a variáveis dinâmicas\n",
    "template = \"\"\"\n",
    "Você é um especialista em Gestão Pública baseada em Evidências.\n",
    "Sua tarefa é classificar um novo texto em uma das etapas do ciclo de políticas públicas, \n",
    "utilizando ESTRITAMENTE o contexto fornecido abaixo.\n",
    "\n",
    "CONTEXTO DO GUIA INSTITUCIONAL:\n",
    "{context}\n",
    "\n",
    "TEXTO PARA CLASSIFICAÇÃO:\n",
    "{question}\n",
    "\n",
    "RESPOSTA ESPERADA:\n",
    "1. Nome da Etapa (Ex: I DEFINIÇÃO, II MOBILIZAÇÃO, etc)\n",
    "2. Justificativa curta (máximo 2 linhas) explicando por que se encaixa nessa etapa.\n",
    "3. Página de referência do guia original.\n",
    "\"\"\"\n",
    "\n",
    "# 3. O ChatPromptTemplate lê o objeto 'tamplate' e substitui as variáveis dinâmicas em seus respectivos lugares\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Pipeline completa \n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} # O RunnablePassthrough() age como um \"placeholder\" que diz à chain: \"Espere até o momento do .invoke() para pegar a query\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # Limpeza: Remove as rebarbas da API e entrega apenas o produto final (Texto).\n",
    ")\n",
    "\n",
    "\n",
    "query_usuario = \"\"\"\n",
    "O orçamento de 2026 prevê cortes severos nas bolsas da CAPES e CNPq, \n",
    "com reduções de até 18% e 25%, respectivamente. A Lei Orçamentária Anual (LOA) \n",
    "de 2026 aponta uma queda de R$ 359,3 milhões para a CAPES. Esses recursos \n",
    "foram redirecionados para emendas parlamentares em ano eleitoral, \n",
    "gerando grande preocupação na comunidade acadêmica quanto à formação de pesquisadores.\n",
    "\"\"\"\n",
    "\n",
    "resposta = chain.invoke(query_usuario)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
