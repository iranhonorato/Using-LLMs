{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205184e5",
   "metadata": {},
   "source": [
    "# 1. Inicializando projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407b0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\"../../.env\")\n",
    "\n",
    "pinecone_api_ley = config[\"PINECONE_API_KEY\"]\n",
    "pinecone_env = config[\"PINECONE_ENVIRONMENT\"]\n",
    "index_pinecone = config[\"INDEX_NAME\"]\n",
    "\n",
    "openai_api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4222c6",
   "metadata": {},
   "source": [
    "## 1.1 Inicializar cliente Pinecone (configurar conexão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e356faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "    \"name\": \"livreto-base-evidencia\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"livreto-base-evidencia-y4phmo4.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"region\": \"us-east-1\",\n",
      "            \"cloud\": \"aws\",\n",
      "            \"read_capacity\": {\n",
      "                \"mode\": \"OnDemand\",\n",
      "                \"status\": {\n",
      "                    \"state\": \"Ready\",\n",
      "                    \"current_shards\": null,\n",
      "                    \"current_replicas\": null\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 1536,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": {\n",
      "        \"embedding_model\": \"text-embedding-3-small\"\n",
      "    }\n",
      "}, {\n",
      "    \"name\": \"ods-onu\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"ods-onu-y4phmo4.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"region\": \"us-east-1\",\n",
      "            \"cloud\": \"aws\",\n",
      "            \"read_capacity\": {\n",
      "                \"mode\": \"OnDemand\",\n",
      "                \"status\": {\n",
      "                    \"state\": \"Ready\",\n",
      "                    \"current_shards\": null,\n",
      "                    \"current_replicas\": null\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 512,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": {\n",
      "        \"embedding_model\": \"text-embedding-3-small\"\n",
      "    }\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_ley)\n",
    "index = pc.Index(index_pinecone)\n",
    "\n",
    "# Verificando se está tudo ok\n",
    "print(pc.list_indexes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9ff9d",
   "metadata": {},
   "source": [
    "# 2. Extrair texto do PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03412fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe? False\n",
      "Caminho absoluto: C:\\Users\\irani\\Desktop\\Personal code\\LLM\\Using LLM\\LangChain\\RAG\\LangChain\\RAG\\Data\\Livreto_BaseEvidencia_2019.11.21.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"LangChain/RAG/Data/Livreto_BaseEvidencia_2019.11.21.pdf\")\n",
    "print(\"Existe?\", path.exists())\n",
    "print(\"Caminho absoluto:\", path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d854c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "path = \"Data/Livreto_BaseEvidencia_2019.11.21.pdf\"\n",
    "pages=[]\n",
    "with pdfplumber.open(path) as pdf:\n",
    "    for i , p in enumerate(pdf.pages, start=1):\n",
    "        text = p.extract_text() or \"\"\n",
    "        pages.append({\"page\": i, \"text\": text})\n",
    "\n",
    "with open(\"data/pdf_pages_livreto.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(pages,f,ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd7470",
   "metadata": {},
   "source": [
    "# Limpeza e normalização de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcb95af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpar_texto(texto: str) -> str:\n",
    "    if not texto:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove caracteres estranhos comuns de OCR\n",
    "    texto = re.sub(r'[^\\wÀ-ÿ\\s.,;:!?()\\-\"“”]', ' ', texto)\n",
    "\n",
    "    # Remove letras soltas em linhas (ex: O\\nI\\nR\\n)\n",
    "    texto = re.sub(r'(?:\\b[A-ZÀ-Ý]\\b\\s*){3,}', ' ', texto)\n",
    "\n",
    "    # Normaliza quebras de linha\n",
    "    texto = re.sub(r'\\n{2,}', '\\n', texto)\n",
    "    texto = re.sub(r'\\n', ' ', texto)\n",
    "\n",
    "    # Normaliza espaços\n",
    "    texto = re.sub(r'\\s{2,}', ' ', texto)\n",
    "\n",
    "    return texto.strip()\n",
    "\n",
    "def limpar_json_paginas(paginas: list) -> list:\n",
    "    json_limpo = []\n",
    "\n",
    "    for pagina in paginas:\n",
    "        texto_limpo = limpar_texto(pagina.get(\"text\", \"\"))\n",
    "\n",
    "        # ignora páginas vazias após limpeza\n",
    "        if texto_limpo:\n",
    "            json_limpo.append({\n",
    "                \"page\": pagina[\"page\"],\n",
    "                \"text\": texto_limpo\n",
    "            })\n",
    "\n",
    "    return json_limpo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd9dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/pdf_pages_livreto.json\", encoding=\"utf-8\") as f:\n",
    "    paginas = json.load(f)\n",
    "\n",
    "json_limpo = limpar_json_paginas(paginas)\n",
    "\n",
    "with open(\"data/pdf_pages_livreto_clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_limpo, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca9aed",
   "metadata": {},
   "source": [
    "# Dividir texto em pedaços menores (Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faedbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dados\n",
    "from langchain_core.documents import Document\n",
    "with open('data/pdf_pages_livreto_clean.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "paginas_texto = {}\n",
    "for item in data:\n",
    "    p = item['page']\n",
    "    t = item['text']\n",
    "    if p not in paginas_texto:\n",
    "        paginas_texto[p] = t\n",
    "    else:\n",
    "        # Une fragmentos da mesma página com uma quebra de linha\n",
    "        paginas_texto[p] += \"\\n\" + t\n",
    "\n",
    "# Convertendo para objetos Document do LangChain (Lista de objetos consolidados)\n",
    "documents = [\n",
    "    Document(page_content=texto, metadata={\"page\": pagina}) \n",
    "    for pagina, texto in paginas_texto.items()\n",
    "]\n",
    "\n",
    "\n",
    "# Estratégia de Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, # Suficiente para captar o tópico inteiro de uma página\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"I DEFINIÇÃO\", \"II MOBILIZAÇÃO\", \"III DETERMINANTES\", \n",
    "                \"IV SOLUÇÃO\", \"V JUSTIFICATIVA\", \"VI APRIMORAMENTO\", \n",
    "                \"VII CERTIFICAÇÃO\", \"\\n\\n\", \". \"]\n",
    ")\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61eb1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conferindo 'a cara' dos chunks!\n",
      "V JUSTIFICATIVA Toda solução demanda recursos. Para ser justificada, uma solução precisa que o valor do que é capaz de alcançar (seus benefícios) supere o valor.dos recursos que necessita (seus custos). Embora a justificativa definitiva de uma solução só possa ocorrer após a sua impantação, a decisão por adotá-la precisa ser baseada em avaliações ex-ante de sua relação custo-efetividade e custo-benefício. AVITACIFITSUJ “If whatever it is you re explaining has some measure ... you ll be much better able to discriminate among competing hypotheses. What is vague ... is open to many explanations” Carl Sagan\n"
     ]
    }
   ],
   "source": [
    "print(\"Conferindo 'a cara' dos chunks!\")\n",
    "print(chunks[13].page_content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69286ce6",
   "metadata": {},
   "source": [
    "# Embeddings + Ingestão no Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef2a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Inicializando o modelo de embeddings da OpenAI\n",
    "# O modelo 'text-embedding-3-small' é o mais custo-benefício atualmente\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=openai_api_key, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4882be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso! 22 documentos foram indexados no Pinecone.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Enviando os chunks (sua lista de Document) para o Pinecone\n",
    "# O LangChain vai automaticamente gerar os embeddings e salvar no Index\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=chunks, # Usando a variável 'chunks'\n",
    "    embedding=embeddings_model,\n",
    "    index_name=index_pinecone,\n",
    "    pinecone_api_key=pinecone_api_ley,\n",
    "    namespace=\"livreto-metricis\"\n",
    ")\n",
    "\n",
    "print(f\"Sucesso! {len(chunks)} documentos foram indexados no Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b709f6",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "127d58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classificação Sugerida ---\n",
      "Conteúdo do Guia: V JUSTIFICATIVA Valoração do custo Impacto (eficácia) Recursos são sempre escassos. Assim, justificar, com base em evidência a adoção de uma ação requer tanto estimativas da magnitude do seu impacto q...\n",
      "Página Original: 16\n"
     ]
    }
   ],
   "source": [
    "# Texto de um novo documento que você deseja classificar\n",
    "query = \"\"\"\n",
    "O orçamento de 2026 prevê cortes severos nas bolsas da CAPES e CNPq, \n",
    "com reduções de até 18% e 25%, respectivamente. A Lei Orçamentária Anual (LOA) \n",
    "de 2026 aponta uma queda de R$ 359,3 milhões para a CAPES. Esses recursos \n",
    "foram redirecionados para emendas parlamentares em ano eleitoral, \n",
    "gerando grande preocupação na comunidade acadêmica quanto à formação de pesquisadores.\"\"\"\n",
    "\n",
    "# Busca os documentos mais parecidos\n",
    "docs = vector_store.similarity_search(query, k=1)\n",
    "\n",
    "if docs:\n",
    "    print(\"\\n--- Classificação Sugerida ---\")\n",
    "    print(f\"Conteúdo do Guia: {docs[0].page_content[:200]}...\")\n",
    "    print(f\"Página Original: {docs[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cb58e",
   "metadata": {},
   "source": [
    "# Consulta e recuperação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b651f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o retriever (configurado para trazer o chunk mais relevante)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# O retriever é um motor de busca.\n",
    "# 1. Ele transforma a sua query_usuario em um vetor.\n",
    "# 2. Ele vai ao Pinecone e procura o chunk que tem a maior \"similaridade de cosseno\" (o texto mais parecido semanticamente).\n",
    "# 3. k=1: Você configurou para ele trazer apenas o melhor resultado. Se o usuário fala de \"orçamento\", o retriever traz a página de custos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea6c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Nome da Etapa: V JUSTIFICATIVA\n",
      "2. Justificativa curta: O texto aborda a alocação e valoração de recursos, destacando a necessidade de justificar cortes orçamentários com base em evidências.\n",
      "3. Página de referência do guia original: 16\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Definimos o modelo (LLM)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key, temperature=0)\n",
    "\n",
    "# 2. Criamos o template com 'espaços vazios' destinados a variáveis dinâmicas\n",
    "template = \"\"\"\n",
    "Você é um especialista em Gestão Pública baseada em Evidências.\n",
    "Sua tarefa é classificar um novo texto em uma das etapas do ciclo de políticas públicas, \n",
    "utilizando ESTRITAMENTE o contexto fornecido abaixo.\n",
    "\n",
    "CONTEXTO DO GUIA INSTITUCIONAL:\n",
    "{context}\n",
    "\n",
    "TEXTO PARA CLASSIFICAÇÃO:\n",
    "{question}\n",
    "\n",
    "RESPOSTA ESPERADA:\n",
    "1. Nome da Etapa (Ex: I DEFINIÇÃO, II MOBILIZAÇÃO, etc)\n",
    "2. Justificativa curta (máximo 2 linhas) explicando por que se encaixa nessa etapa.\n",
    "3. Página de referência do guia original.\n",
    "\"\"\"\n",
    "\n",
    "# 3. O ChatPromptTemplate lê o objeto 'tamplate' e substitui as variáveis dinâmicas em seus respectivos lugares\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Pipeline completa \n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} # O RunnablePassthrough() age como um \"placeholder\" que diz à chain: \"Espere até o momento do .invoke() para pegar a query\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # Limpeza: Remove as rebarbas da API e entrega apenas o produto final (Texto).\n",
    ")\n",
    "\n",
    "\n",
    "query_usuario = \"\"\"\n",
    "O orçamento de 2026 prevê cortes severos nas bolsas da CAPES e CNPq, \n",
    "com reduções de até 18% e 25%, respectivamente. A Lei Orçamentária Anual (LOA) \n",
    "de 2026 aponta uma queda de R$ 359,3 milhões para a CAPES. Esses recursos \n",
    "foram redirecionados para emendas parlamentares em ano eleitoral, \n",
    "gerando grande preocupação na comunidade acadêmica quanto à formação de pesquisadores.\n",
    "\"\"\"\n",
    "\n",
    "resposta = chain.invoke(query_usuario)\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd18cb",
   "metadata": {},
   "source": [
    "# É isso? Dominamos tudo sobre agentes e RAG? \n",
    "\n",
    "# **CLARO QUE NÃO**\n",
    "\n",
    "O que fizemos ali acima, especialmente o conteúdo dessa última célula é o que pode-se chamar de RAG Linear (Chain) mas existem mais coisas para se saber no mundo real... \n",
    "\n",
    "Mas primeiro vamos entender exatamente o que e o RAG Linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e4eec1",
   "metadata": {},
   "source": [
    "# **RAG Linear (Chain)\n",
    "\n",
    "**Definição:**\n",
    "\n",
    "O RAG Linear (ou Retrieval-Augmented Generation de etapa única) é a arquitetura fundamental para sistemas que buscam enriquecer o conhecimento de um modelo de linguagem com dados privados. De maneira didática, podemos compará-lo a uma prova de consulta com tempo limitado.\n",
    "\n",
    "Imagine que você entrega uma pergunta a um assistente, mas ele não tem a resposta na memória. Antes de responder, ele corre até uma estante específica, pega o livro mais relevante, lê um parágrafo e, com base no que leu, redige a resposta.\n",
    "\n",
    "Aqui está o funcionamento técnico dividido em quatro pilares:\n",
    "\n",
    "**1. A Preparação (Indexação):**\n",
    "\n",
    "Antes de qualquer pergunta ser feita, seus documentos passam por um processo de \"tradução\". Fragmentação (Chunking): O texto é quebrado em pedaços menores para não ultrapassar o limite de leitura da IA. Embeddings: Cada pedaço é transformado em uma lista de números (vetores) que representam o seu significado semântico. Vector Store: Esses vetores são armazenados em um banco de dados (como o Pinecone que você configurou), organizados por \"proximidade de assunto\".\n",
    "\n",
    "**2. A Recuperação (Retrieval):**\n",
    "\n",
    "Quando o usuário envia uma query, o sistema não busca por palavras-chave (como no Google antigo), mas por conceitos. A pergunta também é transformada em vetor. O sistema faz um cálculo matemático para encontrar os $k$ fragmentos no banco de dados que possuem os vetores mais parecidos com o da pergunta.\n",
    "\n",
    "**3. O Aumento (Augmentation):** \n",
    "\n",
    "Nesta fase, ocorre a montagem do Prompt Final. O sistema \"embala\" a pergunta do usuário dentro de um envelope de contexto. \"Com base nestes fragmentos de texto: [CONTEXTO RECUPERADO], responda à seguinte pergunta: [PERGUNTA DO USUÁRIO].\" O RunnablePassthrough() que você usou no código serve justamente para garantir que a pergunta passe intacta por esse processo de montagem.\n",
    "\n",
    "**4. A Geração (Generation)**\n",
    "\n",
    "Finalmente, o LLM (como o GPT-4o) recebe esse envelope. Ele não está mais tentando \"adivinhar\" a resposta com base no que aprendeu na internet; ele está agindo como um analista que resume os fatos apresentados no contexto para gerar a resposta final.\n",
    "\n",
    "**Por que ele é chamado de \"Linear\"?**\n",
    "\n",
    "Diferente do Agente (que é circular e pode decidir buscar mais informações se a primeira busca for ruim), o RAG Linear segue uma linha reta:\n",
    "- 1. Recebe Entrada \n",
    "- 2. Busca Contexto \n",
    "- 3. Gera Resposta \n",
    "- 4. Entrega ao Usuário.\n",
    "\n",
    "**Vantagens:**\n",
    "\n",
    "- 1. Velocidade: É extremamente rápido, pois faz apenas uma chamada ao banco de vetores.\n",
    "- 2. Previsibilidade: Você sabe exatamente qual caminho o dado percorreu.\n",
    "\n",
    "**Limitações:** \n",
    "- 1. Se o \"buscador\" (retriever) falhar em encontrar o parágrafo exato na primeira tentativa, o LLM não terá uma segunda chance para procurar melhor. Assim, o LLM terá que \"chutar\" ou alucinar sobre a resposta faltante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118a812",
   "metadata": {},
   "source": [
    "# **RAG Agêntico (Agent)**\n",
    "\n",
    "**Definição:**\n",
    "\n",
    "O RAG Agêntico representa o nível mais avançado de maturidade em sistemas de Recuperação Aumentada por Geração. Enquanto o RAG Linear opera como uma \"linha de montagem\" rígida, o RAG Agêntico funciona como um pesquisador humano dotado de autonomia, senso crítico e ferramentas.\n",
    "\n",
    "De maneira formal e didática, podemos defini-lo através dos seguintes pilares:\n",
    "\n",
    "**1. O Conceito**\n",
    "\n",
    "Da Reação à Ação. A grande diferença reside na capacidade de raciocínio (Reasoning). Em um RAG Agêntico, o modelo de linguagem (LLM) não recebe apenas um comando para responder, mas sim um objetivo a ser alcançado.\n",
    "\n",
    "O Agente atua em um ciclo conhecido como ReAct (Reason + Act):\n",
    "\n",
    "- Pensamento: \"O usuário perguntou sobre cortes no orçamento e ODS. Preciso primeiro entender o que o manual diz sobre ciclos de orçamento.\"\n",
    "\n",
    "- Ação: \"Vou usar a ferramenta busca_gestao_publica.\"\n",
    "\n",
    "- Observação: \"O resultado da busca foi insuficiente. Preciso tentar outro termo ou buscar na ferramenta busca_ods_onu.\"\n",
    "\n",
    "**2. A \"Caixa de Ferramentas\" (Tool Use)**\n",
    "\n",
    "Diferente do modelo linear, onde o banco de dados é \"empurrado\" para a IA, no RAG Agêntico a IA escolhe quando e qual ferramenta usar. Você define funções (tools) que o agente pode chamar:\n",
    "\n",
    "- Ele pode decidir pesquisar no Pinecone.\n",
    "\n",
    "- Pode decidir fazer um cálculo matemático.\n",
    "\n",
    "- Pode decidir formatar o dado final usando um esquema.\n",
    "\n",
    "**3. A Capacidade de Auto-Correção e Iteração**\n",
    "\n",
    "Esta é a característica mais poderosa. O RAG Agêntico é capaz de avaliar a qualidade da informação recuperada:\n",
    "\n",
    "- 1. Busca Multi-etapas: Se uma pergunta complexa exige dados de duas fontes diferentes (ex: o Livreto de Políticas Públicas e o Catálogo de ODS), ele faz a primeira busca, analisa o que encontrou, e só então faz a segunda busca para complementar.\n",
    "\n",
    "- 2. Refinamento: Se a informação recuperada for ambígua, o agente pode decidir pesquisar novamente com palavras-chave diferentes antes de formular a resposta ao usuário.\n",
    "\n",
    "**4. Saída Estruturada e Validação**\n",
    "\n",
    "Ao utilizar o Pydantic, por exemplo, o Agente não apenas escreve um texto, mas trabalha para \"encaixar\" a realidade nos critérios que você definiu. Ele atua como um codificador:\n",
    "\n",
    "Ele lê o texto.\n",
    "\n",
    "- 1. Busca as referências.\n",
    "\n",
    "- 2. Valida se a classificação \"A\" ou \"B\" realmente existe no seu Enum.\n",
    "\n",
    "- 3. Só entrega o resultado quando todos os campos obrigatórios do seu \"Codebook\" estão preenchidos corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc244c6",
   "metadata": {},
   "source": [
    "# **RAG Linear** x **RAG Agêntico**\n",
    "\n",
    "\n",
    "**Comparação Didática: O Estagiário vs. O Analista Sênior**\n",
    "\n",
    "- RAG Linear (Estagiário): Você entrega uma pergunta e uma pasta de documentos. Ele lê o que está no topo da pasta e escreve o que entendeu, sem questionar se aquela informação é suficiente ou se está correta.\n",
    "\n",
    "- RAG Agêntico (Analista Sênior): Você entrega um objetivo (\"Classifique este projeto\"). O analista olha para a estante, escolhe os livros certos, percebe que falta uma informação sobre ODS, vai até outra seção da biblioteca, cruza os dados, verifica se a classificação segue as normas da instituição e entrega um relatório técnico padronizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cae10",
   "metadata": {},
   "source": [
    "# **Casos de uso**\n",
    "\n",
    "### **RAG Linear** \n",
    "\n",
    "O RAG Linear é ideal para situações de alta previsibilidade e baixo nível de ambiguidade.\n",
    "\n",
    "- Atendimento ao Cliente (FAQs Dinâmicos): Responder perguntas diretas baseadas em manuais de produtos ou políticas de reembolso.\n",
    "\n",
    "- Busca em Bases de Documentos Técnicos: Engenheiros que precisam localizar rapidamente uma especificação técnica ou um código de erro em um manual de mil páginas.\n",
    "\n",
    "- Análise de Sentimento com Contexto: Avaliar se um feedback de cliente é positivo ou negativo, fornecendo à IA exemplos reais de guias de estilo da marca como contexto.\n",
    "\n",
    "- Sumarização de Documentos Únicos: Gerar o resumo executivo de um relatório financeiro ou de uma ata de reunião que acabou de ser carregada no sistema. \n",
    "\n",
    "### **RAG Agêntico**\n",
    "\n",
    "- Revisão Sistemática de Literatura: Quando o sistema precisa classificar um estudo científico, ele usa ferramentas para consultar o Codebook (Livreto), depois consulta as ODS e, por fim, valida se a classificação faz sentido antes de gerar o JSON final.\n",
    "\n",
    "- Análise de Conformidade Legal (Compliance): Um agente que recebe um novo contrato e precisa verificar se ele fere cláusulas em diferentes legislações (trabalhista, ambiental, tributária), realizando buscas específicas para cada \"frente\" jurídica.\n",
    "\n",
    "- Suporte Técnico Avançado (Troubleshooting): Diferente do FAQ simples, aqui o agente pode pedir informações adicionais ao usuário (\"Qual a luz que está piscando?\") e, com base na resposta, decidir qual parte do esquema técnico deve investigar a seguir.\n",
    "\n",
    "- Relatórios de Inteligência de Mercado: Um agente que precisa consolidar dados de notícias recentes, relatórios internos de vendas e dados de concorrentes para produzir uma análise de risco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62af1df",
   "metadata": {},
   "source": [
    "# Exemplo de **RAG Agêntico**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8546eba",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# -------------------------- Definindo vectors databases --------------------------\n",
    "vector_store_ods = PineconeVectorStore.from_documents(\n",
    "    documents=chunks_ods,\n",
    "    embedding=embeddings_model,\n",
    "    index_name=index_pinecone, # O mesmo índice que você já usa\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    "    namespace=\"catalogo-ods\"    # A separação mágica acontece aqui\n",
    ")\n",
    "\n",
    "vector_store_livreto = PineconeVectorStore(\n",
    "    index_name=\"livreto-base-evidencia\",\n",
    "    embedding=embeddings_model,\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    ")\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------- Definindo retrievals -------------------------------------\n",
    "retriever_livreto = vector_store_livreto.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"namespace\": \"livreto-metricis\", \n",
    "        \"k\": 3\n",
    "        }\n",
    "    ) \n",
    "\n",
    "retriever_ods = vector_store_ods.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"namespace\": \"catalogo-ods\",\n",
    "        \"k\": 2\n",
    "        }\n",
    "    )\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------- Definindo tools ------------------------------\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Aqui, você transforma os retrievers e uma função Python em ferramentas que o Agente sabe usar:\n",
    "tool_livreto = create_retriever_tool(\n",
    "    retriever_livreto,\n",
    "    \"busca_gestao_publica\",\n",
    "    \"Útil para identificar qual etapa do ciclo de políticas públicas se relaciona com um texto ou projeto.\"\n",
    ")\n",
    "\n",
    "tool_ods = create_retriever_tool(\n",
    "    retriever_ods,\n",
    "    \"busca_ods_onu\",\n",
    "    \"Útil para identificar quais Objetivos de Desenvolvimento Sustentável (ODS) e metas da ONU se relacionam com um texto ou projeto.\"\n",
    ")\n",
    "\n",
    "# tool_livreto e tool_ods: Permitem que o Agente \"saia para pesquisar\" se não tiver certeza sobre uma ODS ou uma etapa do ciclo de políticas.\n",
    "\n",
    "from enum import Enum\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import List\n",
    "\n",
    "class EtapaCicloPP(Enum):\n",
    "    DEFINICAO = \"Definição e Dimensão\"\n",
    "    MOBILIZACAO = \"Mobilização\"\n",
    "    MAPEAMENTO = \"Mapeamento dos Determinantes\"\n",
    "    SOLUCAO = \"Solução\"\n",
    "    JUSTIFICATIVA = \"Justificativa\"\n",
    "    APRIMORAMENTO = \"Aprimoramento\"\n",
    "    CERTIFICACAO = \"Certificação\"\n",
    "\n",
    "class Classificacao(Enum):\n",
    "    ACADEMICA = \"Acadêmica\"\n",
    "    TECNICA = \"Técnica\"\n",
    "\n",
    "class AreaAvaliada(Enum):\n",
    "    EDUCACAO = \"Educação\"\n",
    "    SAUDE = \"Saúde\"\n",
    "    MEIO_AMBIENTE = \"Meio Ambiente\"\n",
    "    GENERO = \"Gênero\"\n",
    "    RACA = \"Raça\"\n",
    "    POBREZA = \"Pobreza\"\n",
    "    DESENVOLVIMENTO_SOCIAL = \"Desenvolvimento Social\"\n",
    "\n",
    "\n",
    "class Metodologia(Enum):\n",
    "    QUALITATIVA = \"Qualitativa\"\n",
    "    QUANTITATIVA = \"Quantitativa\"\n",
    "    MISTA = \"Mista\"\n",
    "\n",
    "\n",
    "class Trabalho(BaseModel):\n",
    "    classificacao: Classificacao = Field(..., description=\"Acadêmico ou técnico\")\n",
    "    metodologia: Metodologia = Field(..., description=\"Abordagem do trabalho\")\n",
    "    area_avaliada: AreaAvaliada = Field(..., description=\"Área avaliada no trabalho\")\n",
    "    ods: List[int] = Field(..., description=\"Número das ODS relacionadas ao trabalho (texto)\")\n",
    "    etapa: EtapaCicloPP = Field(..., description=\"Etapa do ciclo de políticas públicas\")\n",
    "    titulo:str = Field(..., description=\"Titulo do trabalho\")\n",
    "\n",
    "\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 1. Criamos uma função decorada como @tool\n",
    "# Isso transforma seu Pydantic em uma ferramenta que o Agente reconhece\n",
    "@tool\n",
    "def formatar_resposta_final(analise: Trabalho):\n",
    "    \"\"\"\n",
    "    Usa esta ferramenta por último para entregar a análise final estruturada. \n",
    "    Esta ferramenta valida todos os Enums e campos obrigatórios.\n",
    "    \"\"\"\n",
    "    return analise\n",
    "tools_com_output = [tool_livreto, tool_ods, formatar_resposta_final]\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------- Criando agente --------------------------------\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "def agente_classificador(query:str) -> dict:\n",
    "    # 1. O system_text: É a \"Constituição\" do Agente. Aqui você define o Codebook.\n",
    "    #  Você deu à IA a \"personalidade\" de um pesquisador sênior e explicou cada regra de negócio do Insper. \n",
    "    system_text = \"\"\"\n",
    "        Você é um pesquisador sênior especializado em revisão sistemática de políticas públicas e \n",
    "        nos Objetivos de Desenvolvimento Sustentável (ODS) da ONU. \n",
    "        Sua tarefa é codificar estudos extraindo dados sistematicamente conforme os critérios abaixo.\n",
    "\n",
    "        ### CRITÉRIOS DE CODIFICAÇÃO (CODEBOOK)\n",
    "\n",
    "        1. CLASSIFICAÇÃO:\n",
    "        - \"Academica\": Foco em rigor metodológico, fundamentação teórica e revisão de literatura.\n",
    "        - \"Tecnica\": Foco em execução, procedimentos operacionais e resultados práticos/gestão.\n",
    "\n",
    "        2. METODOLOGIA:\n",
    "        - \"Qualitativa\", \"Quantitativa\" ou \"Mista\".\n",
    "\n",
    "        3. ÁREA AVALIADA:\n",
    "        - Escolha uma: \"Educação\", \"Saúde\", \"Meio Ambiente\", \"Gênero\", \"Raça\", \"Pobreza\" ou \"Desenvolvimento Social\".\n",
    "\n",
    "        4. ETAPAS DO CICLO DE POLÍTICAS PÚBLICAS (Base: Insper/IAS 2019):\n",
    "        - \"Definição e Dimensão\": Especificação, mensuração e consequências do problema.\n",
    "        - \"Mobilização\": Sensibilização, percepção de atores-chave e eficácia da mobilização.\n",
    "        - \"Mapeamento dos Determinantes\": Identificação e priorização de causas modificáveis.\n",
    "        - \"Solução\": Estratégia, modelo de mudança, validade das hipóteses e metas.\n",
    "        - \"Justificativa\": Descrição de impacto, valoração de custos/benefícios e custo-efetividade.\n",
    "        - \"Aprimoramento\": Monitoramento, eficiência, eficácia alocativa e validação de implementação.\n",
    "        - \"Certificação\": Estimativas finais de impacto, custo final, adequação e resolutividade.\n",
    "\n",
    "        5. ODS (Objetivos de Desenvolvimento Sustentável):\n",
    "        - Identifique as ODS (1 a 17) mais relacionadas à temática central.\n",
    "        - Exemplo de formato: \"ODS 1; ODS 5; ODS 10\".\n",
    "\n",
    "        ### TAREFA\n",
    "        Sempre que receber um texto, analise-o profundamente contra os critérios acima e \n",
    "        responda **APENAS** com um objeto JSON válido. Não inclua textos explicativos fora do JSON.\n",
    "\n",
    "        ### FORMATO DE SAÍDA (OBRIGATÓRIO)\n",
    "        {{\n",
    "            \"titulo\": \"Título original do trabalho\",\n",
    "            \"classificacao\": \"Academica ou Tecnica\",\n",
    "            \"metodologia\": \"Qualitativa, Quantitativa ou Mista\",\n",
    "            \"area_avaliada\": \"Área correspondente\",\n",
    "            \"etapa\": \"Nome da etapa conforme o Codebook\",\n",
    "            \"ods\": \"Lista de ODS (Ex: 3; 4)\",\n",
    "            \"resumo_justificativa\": \"Breve frase justificando a etapa do ciclo escolhida\"\n",
    "        }}\n",
    "    \"\"\"\n",
    "    # Note o uso de chaves duplas {{ }} no formato de saída; \n",
    "    # isso é necessário porque o Python entende chaves únicas como variáveis, \n",
    "    # então \"escapamos\" elas para que o Agente entenda que é um texto de estrutura JSON.\n",
    "\n",
    "\n",
    "    # 2. Definimos o modelo (GPT-4o é o melhor para raciocínio técnico)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key=openai_api_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Criamos o \"Manual de Instruções\" do Agente (Prompt)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_text), # Injeta as regras de pesquisador sênior.\n",
    "        (\"human\", \"{input}\"), # É onde o texto do seu trabalho entrará.\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Esta é a linha mais \"mágica\". \n",
    "        # O scratchpad (bloco de notas) é um espaço dinâmico onde \n",
    "        # o LangChain armazena o histórico de quais ferramentas o agente já usou e \n",
    "        # o que ele descobriu nelas antes de te dar a resposta final.\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Construímos de fato o Agent\n",
    "    agent = create_openai_functions_agent(llm, tools_com_output, prompt)\n",
    "    # Esta linha \"cola\" as três partes: \n",
    "    # - o cérebro (llm) \n",
    "    # - as mãos (tools_com_output que definimos antes, como o retriever e a formatação) \n",
    "    # - as instruções (prompt). Ela cria o raciocínio lógico que sabe quando chamar uma função externa.\n",
    "\n",
    "    # 5. Criamos o Executor: O Agente em si é apenas uma lógica; o Executor é quem realmente roda o loop \"Pensar -> Agir -> Observar\".\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=tools_com_output, \n",
    "        verbose=True, # com verbose=True para você ver ele 'pensando'\n",
    "        return_intermediate_steps=False # Queremos apenas o resultado final\n",
    "    )\n",
    "\n",
    "    # 6. Execução: O dicionário {\"input\": query} preenche aquele campo {input} que definimos no prompt lá atrás.\n",
    "    resposta = agent_executor.invoke({\"input\": query}) \n",
    "\n",
    "    return resposta[\"output\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
